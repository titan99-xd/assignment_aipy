{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "gPq4qhpyo1aM",
        "outputId": "fc30c44c-aa93-4f8f-e65d-686c3d06e5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix Entropy: [[47  6]\n",
            " [ 6 21]]\n",
            "Classification Report Entropy:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89        53\n",
            "           1       0.78      0.78      0.78        27\n",
            "\n",
            "    accuracy                           0.85        80\n",
            "   macro avg       0.83      0.83      0.83        80\n",
            "weighted avg       0.85      0.85      0.85        80\n",
            "\n",
            "Confusion Matrix Gini: [[47  6]\n",
            " [ 4 23]]\n",
            "Classification Report Gini:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.89      0.90        53\n",
            "           1       0.79      0.85      0.82        27\n",
            "\n",
            "    accuracy                           0.88        80\n",
            "   macro avg       0.86      0.87      0.86        80\n",
            "weighted avg       0.88      0.88      0.88        80\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nHere, entropy produced 6 false positives, and 6 false negatives. Whereas, gini produced 6 false positives and 4 false negatives. From this it is seen that gini is better at identifying class 1.\\nIn case of class 0, both the models have performed identically producing 6 false positives.\\n\\nOverall gini has performed better as shown by 88% accuracy compared to that of Entropy's 85%.\\n\\nGini is more precise for negative (0) class and for class 1, it is slightly better than entropy (0.79 vs 0.78)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "df = pd.read_csv('/content/sample_data/suv.csv')\n",
        "\n",
        "# feature variable X ['Age','EstimatedSalary']\n",
        "X = df[['Age','EstimatedSalary']]\n",
        "\n",
        "# target variable y ['Purchased']\n",
        "y = df['Purchased']\n",
        "\n",
        "# split dataset into 80-20\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
        "\n",
        "# Scale feature using StandardScaler\n",
        "scaler  = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# train model using DecisionTreeClassifier with entropy\n",
        "dt_entropy = DecisionTreeClassifier(criterion='entropy', random_state=5)\n",
        "dt_entropy.fit(X_train, y_train)\n",
        "y_pred_entropy = dt_entropy.predict(X_test)\n",
        "\n",
        "\n",
        "# Confusion matrix and classification report\n",
        "dt_entropy_conf = confusion_matrix(y_test, y_pred_entropy)\n",
        "print(f'Confusion Matrix Entropy: {dt_entropy_conf}')\n",
        "\n",
        "dt_entropy_class = classification_report(y_test, y_pred_entropy)\n",
        "print(f'Classification Report Entropy: {dt_entropy_class}')\n",
        "\n",
        "\n",
        "\n",
        "# Again train model using DecisionTreeClassifier with gini\n",
        "dt_gini = DecisionTreeClassifier(criterion='gini')\n",
        "dt_gini.fit(X_train, y_train)\n",
        "y_pred_gini = dt_gini.predict(X_test)\n",
        "\n",
        "# Confusion matrix and classification report gini\n",
        "dt_gini_conf = confusion_matrix(y_test, y_pred_gini)\n",
        "print(f'Confusion Matrix Gini: {dt_gini_conf}')\n",
        "\n",
        "dt_gini_class = classification_report(y_test, y_pred_gini)\n",
        "print(f'Classification Report Gini: {dt_gini_class}')\n",
        "\n",
        "\n",
        "'''\n",
        "Here, entropy produced 6 false positives, and 6 false negatives. Whereas, gini produced 6 false positives and 4 false negatives. From this it is seen that gini is better at identifying class 1.\n",
        "In case of class 0, both the models have performed identically producing 6 false positives.\n",
        "\n",
        "Overall gini has performed better as shown by 88% accuracy compared to that of Entropy's 85%.\n",
        "\n",
        "Gini is more precise for negative (0) class and for class 1, it is slightly better than entropy (0.79 vs 0.78)\n",
        "'''"
      ]
    }
  ]
}